#### Lecture slides: [click here](https://yadi.sk/i/ibh5co-d3N6EM5)
__Note__: Seminars assume that you remember batch normalization and dropout from last lecture. If you don't, look into week1.

## Materials
- [russian] Convolutional networks - [video](https://yadi.sk/i/hDIkaR4H3EtnXM)
- [english] Convolutional networks (karpathy) - [video](https://www.youtube.com/watch?v=AQirPKrAyDg)

- Reading
  - http://cs231n.github.io/convolutional-networks/
  - http://cs231n.github.io/understanding-cnn/
  - [a deep learning neophite cheat sheet](http://www.kdnuggets.com/2016/03/must-know-tips-deep-learning-part-1.html)
  - [more stuff for vision](https://bavm2013.splashthat.com/img/events/46439/assets/34a7.ranzato.pdf)
  - a [CNN trainer in a browser](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)


## Assignment

__Theano track__: seminar_theano.ipynb
__Tensorflow track__: seminar_tf_keras.ipynb

If you're on main [theano] track, please do take a look at seminar_tf_keras.ipynb at least diagonally. It uses keras, a framework that can use both theano and tf. Unlike lasagne, keras makes it super-simple to train feedforward networks in almost scikit-learn fashion. Lasagne, on the contrary, starts to shine in more complicated cases which we'll cover later in our course.
