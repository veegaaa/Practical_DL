__[slides](https://yadi.sk/i/mh1SOEKp3PXfuU)__


### Materials
- Russian [lecture](https://www.youtube.com/watch?v=_XRBlhzb31U), [seminar](https://www.youtube.com/watch?v=s3oONja_SNQ)
- The only more-or-less reasonable lecture we found: [video](https://www.youtube.com/watch?v=QuELiw8tbx8). Contributions welcome :)

### More materials
- Gumbel-softmax (usable for semi-hard attention) - [video](https://www.youtube.com/watch?v=wVkLM2KKHp8), [tutorial](http://blog.evjang.com/2016/11/tutorial-categorical-variational.html), [arxiv](https://arxiv.org/abs/1611.01144)
- Attention is all you need - on seq2seq architecture made entirely from attention - [video](https://www.youtube.com/watch?v=rBCqOTEfxvg), [arxiv](https://arxiv.org/abs/1706.03762)
- Convolutional + attentive seq2seq - [arxiv](https://arxiv.org/pdf/1705.03122.pdf)
- Recurrent dropout: [variational](https://arxiv.org/abs/1512.05287), [gate-specific](https://arxiv.org/abs/1603.05118)
- Normalization: [gate-specific](https://arxiv.org/abs/1603.09025), [layer-norm](https://arxiv.org/abs/1607.06450)
- Improved softmax operators: [post](http://sebastianruder.com/word-embeddings-softmax/index.html)
- Better tokenization for language-related stuff: [arxiv](https://arxiv.org/abs/1508.07909), [code](https://github.com/rsennrich/subword-nmt)

### Homework assignment

There's a main notebook about image captioning (both theano and tf) that's worth 10 points and also an optional attention tutorial (theano only) worth 3 points if submitted.

Helper urls:
- MS COCO challenge: http://mscoco.org/dataset/#overview
- Im2latex: https://openai.com/requests-for-research/#im2latex

