{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford question answering dataset (SQuAD)\n",
    "\n",
    "Today we are going to work with a popular NLP dataset.\n",
    "\n",
    "Here is the description of the original problem:\n",
    "\n",
    "```\n",
    "Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\n",
    "```\n",
    "\n",
    "\n",
    "We are not going to solve it :) Instead we will try to answer the question in a different way: given the question, we will find a **sentence** containing the answer, but not within the context, but in a **whole databank**\n",
    "\n",
    "Just watch the hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open('train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code here is very similar to `week5/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "\n",
    "\n",
    "for q in tqdm.tqdm_notebook(data['data']):\n",
    "    for p in q['paragraphs']:\n",
    "        token_counts.update(tokenize(p['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 4\n",
    "\n",
    "tokens = [w for w, c in token_counts.items() if c > min_count] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_size = len(tokens)+2\n",
    "id_to_word = dict()\n",
    "word_to_id = dict()\n",
    "\n",
    "token_to_id = {t:i+2 for i,t in enumerate(tokens)}\n",
    "\n",
    "id_to_token = {i+2:t for i,t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[id_to_token[42]]==42\n",
    "assert len(token_to_id)==len(tokens)\n",
    "assert 0 not in id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def build_dataset(train_data):\n",
    "    '''Takes SQuAD data\n",
    "    Returns a list of tuples - a set of pairs (q, a_+)\n",
    "    '''\n",
    "    D = []\n",
    "    for q in tqdm.tqdm_notebook(train_data):\n",
    "        for p in q['paragraphs']:\n",
    "            offsets = []\n",
    "            curent_index = 0\n",
    "            for sent in sent_tokenize(p['context']):\n",
    "                curent_index+=len(sent)+2\n",
    "                offsets.append((curent_index, sent))\n",
    "                \n",
    "                \n",
    "            for qa in p['qas']:\n",
    "                answer = qa['answers'][0]\n",
    "                found = False\n",
    "                for o, sent in offsets:\n",
    "                    if answer['answer_start']<o:\n",
    "                        D.append((qa['question'], sent))\n",
    "                        found = True\n",
    "                        break\n",
    "                assert found\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(data['data'], test_size=0.1)\n",
    "\n",
    "Dtrain = build_dataset(train_data)\n",
    "Dval = build_dataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dval[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, UNK):\n",
    "    '''This function gets a string array and transforms it to padded token matrix\n",
    "    Remember to:\n",
    "     - Transform a string to list of tokens\n",
    "     - Transform each token to it ids (if not in the dict, replace with UNK)\n",
    "     - Pad each line to max_len'''\n",
    "    max_len = 0\n",
    "    token_matrix = []\n",
    "    \n",
    "    #<your code>\\\n",
    "    num_tokens = []\n",
    "    \n",
    "    for s in strings:\n",
    "        seq = []\n",
    "        for token in tokenize(s):\n",
    "            if token in token_to_id:\n",
    "                seq.append(token_to_id[token])\n",
    "            else:\n",
    "                seq.append(UNK)\n",
    "        token_matrix.append(seq)\n",
    "        max_len = max(max_len, len(token_matrix[-1]))\n",
    "    # empty batch plug\n",
    "    if max_len == 0:\n",
    "        max_len = 1\n",
    "    for i in range(len(token_matrix)):\n",
    "        num_tokens.append(len(token_matrix[i]))\n",
    "        while(len(token_matrix[i])<max_len):\n",
    "            token_matrix[i].append(0)\n",
    "            \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    return np.array(token_matrix), np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = vectorize([\"Hello, adshkjasdhkas, world\", \"data\"], token_to_id, 1)[0]\n",
    "assert test.shape==(2,3)\n",
    "assert (test[:,1]==(1,0)).all()\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "The beginning is same as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES = \"\"\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_space_dim = 50 # Here we define dimension of the target space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_size = 50\n",
    "word_embeddings_matrix = <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_signle_input(name):\n",
    "    '''Returns a pair of inputs'''\n",
    "    return (tf.placeholder(dtype=tf.int32,shape=[None, None], name=\"word_ids_%s\"%name),\n",
    "    tf.placeholder(dtype=tf.int32,shape=[None], name=\"num_words_%s\"%name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(word_ids, num_words,name, reuse=False):\n",
    "    '''The function takes:\n",
    "     - word_ids - a matrix with word ids\n",
    "     - num_words - a vector, showing how many words is in each sample\n",
    "     - name - name for variables\n",
    "     - reuse - are weights reused\n",
    "     Returns:\n",
    "     - outputs - a matrix [batch_size, target_space_dim]\n",
    "    '''\n",
    "    <YOUR CODE>\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a single `encode`, but with different weights. You can use different encode for anchor and negatives/positives.\n",
    "\n",
    "Negative sampling can be either `in-graph` or `out-graph`. We start with out-graph. In the home assignment you are going to use in-graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_semihard_outputs(anchor_output, positive_output):\n",
    "    \"\"\"Function samples negatives in-graph. Returns negative_output. Use it in home assignment\"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {name: get_signle_input(name) for name in ['anchor', 'positive', 'negative']}\n",
    "margin = 0.1\n",
    "anchor_output = encode(*inputs['anchor'], 'anchor')\n",
    "positive_output = <YOUR CODE>\n",
    "negative_output = <YOUR CODE>\n",
    "\n",
    "positive_dot = <YOUR CODE>\n",
    "negative_dot = <YOUR CODE>\n",
    "\n",
    "loss = <YOUR CODE>\n",
    "recall = tf.reduce_mean(tf.cast(tf.greater(positive_dot, negative_dot), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "def iterate_batches(data, only_positives=False):\n",
    "    \"\"\"Takes a D\n",
    "    Returns a dict, containing pairs for each input type\n",
    "    only_positives indicates either we need to iterate over triplets vs only positive (needed for index)\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        batch = dict()\n",
    "        data_batch = data[i:i+batch_size]\n",
    "        \n",
    "        \n",
    "        batch['positive'] = vectorize([sample[1] for sample in data_batch], token_to_id, 1)\n",
    "        if not only_positives:\n",
    "            <YOUR CODE>\n",
    "        \n",
    "       \n",
    "        \n",
    "        yield batch\n",
    "        i+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer() # <your code here>\n",
    "global_step = tf.Variable(initial_value=0)\n",
    "train_op = optimizer.minimize(\n",
    "  loss=loss,\n",
    "  global_step=global_step, var_list=tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list(iterate_batches(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(batch):\n",
    "    feed_dict = {}\n",
    "    for name, tensors in batch.items():\n",
    "        feed_dict[inputs[name][0]] = tensors[0]\n",
    "        feed_dict[inputs[name][1]] = tensors[1]\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    total_loss, total_recall = 0, 0\n",
    "    batches = 0\n",
    "    for  batch in  iterate_batches(Dval):\n",
    "        batches+=1\n",
    "        current_loss, current_recall =  sess.run([loss, recall], get_inputs(batch))\n",
    "        total_loss+=current_loss\n",
    "        total_recall+=current_recall\n",
    "    total_loss/=batches\n",
    "    total_recall/=batches\n",
    "    if total_recall > 0.9:\n",
    "        print('Cool! If recall is right, you earned (3 pts)')\n",
    "    return (total_loss, total_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for j in range(num_epochs):\n",
    "    for i, (batch) in  enumerate(iterate_batches(Dtrain)):\n",
    "        _, step, current_loss, current_recall =  sess.run([train_op, global_step, loss, recall], get_inputs(batch))\n",
    "        print(\"Current step: %s. Current loss is %s, Current recall is %s\" % (step, current_loss, current_recall))\n",
    "        if i%100==0:\n",
    "            print(\"Validation. Loss: %s, Recall: %s\" %validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Index(object):\n",
    "    \"\"\"Represents index of calculated embeddings\"\"\"\n",
    "    def __init__(self, D):\n",
    "        \"\"\"Class constructor takes a dataset and stores all unique sentences and their embeddings\"\"\"\n",
    "        <YOUR CODE>\n",
    "        \n",
    "    def predict(self, query, top_size =1):\n",
    "        \"\"\"\n",
    "        Function takes:\n",
    "         - query is a string, containing question\n",
    "        Function returns:\n",
    "         - a list with len of top_size, containing the closet answer from the index\n",
    "        You may want to use np.argpartition\n",
    "          \"\"\"\n",
    "        <YOUR CODE>\n",
    "    \n",
    "    def calculate_FHS(self, D):\n",
    "        \"\"\"Prototype for home assignment. Returns a float number\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = Index(Dval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(index.vectors) == len(index.sent)\n",
    "assert type(index.sent[1])==str\n",
    "assert index.vectors.shape == (len(index.sent), target_space_dim)\n",
    "p  = index.predict(\"Hey\", top_size=3)\n",
    "assert len(p) == 3\n",
    "assert type(p[0])==str\n",
    "assert index.predict(\"Hello\", top_size=50)!=index.predict(\"Not Hello\", top_size=50)\n",
    "print(\"Ok (2 pts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index.predict('To show their strength in the international Communist movement, what did China do?', top_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dval[np.random.randint(0, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Home assignment\n",
    "**Task 1.** (3 pts) Implement **semihard** sampling strategy. Use **in-graph** sampling. You have a prototype above\n",
    "\n",
    "**Task 2.1.** (1 pt) Calculate a **FHS** (First Hit Success) metric on a whole validation dataset (over each query on whole `Dval` index). Prototype of the function in in `Index` class. Compare different model based on this metric. Add table with FHS values to your report.\n",
    "\n",
    "**Task 2.2.** Add calculation of other representative metrics. You may want to calculate different recalls on a mini-batch, or some ranking metrics.   \n",
    "\n",
    "**Task 3.** (2 pt) Do experiments with deep architecture and find out the best one. Analyse your results and write a conclusion. \n",
    "\n",
    "**describe your results here**\n",
    "\n",
    "Bonus task 1. (2++ pts) Add manual negatives to the model. What can be a good manual negative in this case?\n",
    "\n",
    "Bonus task 2. (2++ pts) Implement more efficient Nearest Neighbors Search method. How well it performs on our dataset?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "69ee5b52104d471ca7bfb32ba4309743": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "7b18f460e231498eaafa7653026e98e0": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
